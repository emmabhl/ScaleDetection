{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bc39ed80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56abccb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ref_dir = Path(\"atypical_examples\")\n",
    "grouped_imgs = {}\n",
    "for category_dir in ref_dir.iterdir():\n",
    "    if not category_dir.is_dir() or category_dir.name.startswith(\".\"):\n",
    "        continue\n",
    "    img = []\n",
    "    for img_path in category_dir.iterdir():\n",
    "        if not img_path.is_file() or img_path.name.startswith(\".\"):\n",
    "            continue\n",
    "        img.append(img_path)\n",
    "    grouped_imgs[category_dir.name] = img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eeb8ae0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- CONFIG ---\n",
    "IMAGES_DIR = \"images\"         # folder with your 10 images\n",
    "OUTPUT_DIR = \"output\"         # will be created, annotated images saved here\n",
    "REFERENCE_INDEX = 0           # which image to use as reference (0 = first)\n",
    "ORB_N_FEATURES = 3000\n",
    "MATCH_DISTANCE_THRESHOLD = 60  # lower stricter; tune if needed\n",
    "DBSCAN_EPS = 30               # pixels, tune with image resolution\n",
    "DBSCAN_MIN_SAMPLES = 3\n",
    "BOX_MARGIN = 10               # pixels to expand bounding box\n",
    "# ----------------\n",
    "\n",
    "# Convert to grayscale\n",
    "grays = [cv2.cvtColor(im, cv2.COLOR_BGR2GRAY) for im in images]\n",
    "h, w = grays[0].shape[:2]\n",
    "\n",
    "# Create ORB and detect keypoints/descriptors for reference image\n",
    "orb = cv2.ORB_create(nfeatures=ORB_N_FEATURES)\n",
    "kp_ref, des_ref = orb.detectAndCompute(grays[REFERENCE_INDEX], None)\n",
    "if des_ref is None or len(kp_ref) == 0:\n",
    "    raise SystemExit(\"No features found in reference image. Try increasing ORB_N_FEATURES.\")\n",
    "\n",
    "# Match each other image to reference and collect reference keypoint locations for good matches\n",
    "all_ref_pts = []\n",
    "bf = cv2.BFMatcher(cv2.NORM_HAMMING, crossCheck=True)\n",
    "\n",
    "for i, g in enumerate(grays):\n",
    "    if i == REFERENCE_INDEX:\n",
    "        continue\n",
    "    kp, des = orb.detectAndCompute(g, None)\n",
    "    if des is None or len(kp) == 0:\n",
    "        print(f\"Warning: no features in image {image_files[i]} - skipping.\")\n",
    "        continue\n",
    "    # match descriptors: query = reference descriptors, train = current image descriptors\n",
    "    matches = bf.match(des_ref, des)\n",
    "    # keep good matches by distance\n",
    "    good = [m for m in matches if m.distance < MATCH_DISTANCE_THRESHOLD]\n",
    "    # optionally take top-K if too many\n",
    "    good = sorted(good, key=lambda x: x.distance)[:1000]\n",
    "\n",
    "    # For each good match, get the location of the keypoint in the reference image\n",
    "    for m in good:\n",
    "        q_idx = m.queryIdx  # index in des_ref / kp_ref\n",
    "        pt = kp_ref[q_idx].pt  # (x, y) in reference image coordinates (float)\n",
    "        all_ref_pts.append(pt)\n",
    "\n",
    "all_ref_pts = np.array(all_ref_pts)  # shape (N, 2)\n",
    "if all_ref_pts.shape[0] == 0:\n",
    "    raise SystemExit(\"No good matches found across images. Try relaxing MATCH_DISTANCE_THRESHOLD or ORB_N_FEATURES.\")\n",
    "\n",
    "# Cluster the reference points to find the densest region (object location)\n",
    "clust = DBSCAN(eps=DBSCAN_EPS, min_samples=DBSCAN_MIN_SAMPLES).fit(all_ref_pts)\n",
    "labels = clust.labels_\n",
    "unique, counts = np.unique(labels[labels >= 0], return_counts=True)  # ignore noise label -1\n",
    "\n",
    "if len(unique) == 0:\n",
    "    # fallback: use bounding box around all matches\n",
    "    print(\"No DBSCAN clusters found - falling back to bounding box around all matched points.\")\n",
    "    x_min = int(np.min(all_ref_pts[:, 0]))\n",
    "    y_min = int(np.min(all_ref_pts[:, 1]))\n",
    "    x_max = int(np.max(all_ref_pts[:, 0]))\n",
    "    y_max = int(np.max(all_ref_pts[:, 1]))\n",
    "else:\n",
    "    # choose the largest cluster\n",
    "    largest_label = unique[np.argmax(counts)]\n",
    "    mask = labels == largest_label\n",
    "    cluster_pts = all_ref_pts[mask]\n",
    "    x_min = int(np.min(cluster_pts[:, 0]))\n",
    "    y_min = int(np.min(cluster_pts[:, 1]))\n",
    "    x_max = int(np.max(cluster_pts[:, 0]))\n",
    "    y_max = int(np.max(cluster_pts[:, 1]))\n",
    "\n",
    "# Expand box a bit\n",
    "x_min = max(0, x_min - BOX_MARGIN)\n",
    "y_min = max(0, y_min - BOX_MARGIN)\n",
    "x_max = min(w - 1, x_max + BOX_MARGIN)\n",
    "y_max = min(h - 1, y_max + BOX_MARGIN)\n",
    "box = (x_min, y_min, x_max, y_max)\n",
    "print(\"Detected bounding box (x_min, y_min, x_max, y_max):\", box)\n",
    "\n",
    "# Save visualization: plot matched points density and cluster\n",
    "plt.figure(figsize=(6, 6))\n",
    "plt.imshow(cv2.cvtColor(images[REFERENCE_INDEX], cv2.COLOR_BGR2RGB))\n",
    "if all_ref_pts.size:\n",
    "    plt.scatter(all_ref_pts[:, 0], all_ref_pts[:, 1], s=6, c='yellow', alpha=0.6)\n",
    "if 'cluster_pts' in locals():\n",
    "    plt.scatter(cluster_pts[:, 0], cluster_pts[:, 1], s=12, c='red', alpha=0.8)\n",
    "rect = plt.Rectangle((x_min, y_min), x_max - x_min, y_max - y_min,\n",
    "                     linewidth=2, edgecolor='cyan', facecolor='none')\n",
    "plt.gca().add_patch(rect)\n",
    "plt.title('Reference image with matched points and detected box')\n",
    "plt.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(OUTPUT_DIR, 'reference_matched_points_box.png'), dpi=150)\n",
    "plt.close()\n",
    "\n",
    "# Apply the same box to all images and save\n",
    "for fname, im in zip(image_files, images):\n",
    "    im_out = im.copy()\n",
    "    cv2.rectangle(im_out, (x_min, y_min), (x_max, y_max), (0, 255, 255), 2)\n",
    "    # optionally crop & save cropped object\n",
    "    cropped = im[y_min:y_max, x_min:x_max]\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"boxed_{fname}\"), im_out)\n",
    "    cv2.imwrite(os.path.join(OUTPUT_DIR, f\"crop_{fname}\"), cropped)\n",
    "\n",
    "print(\"Saved annotated images and crops to:\", OUTPUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a921d3ec",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (2983,3961) (2736,3648) ",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 24\u001b[39m\n\u001b[32m     22\u001b[39m             correlated_images[(category, img_path.name)] = correlation\n\u001b[32m     23\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m correlated_images\n\u001b[32m---> \u001b[39m\u001b[32m24\u001b[39m correlated_results = \u001b[43mcross_correlate_images\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrouped_imgs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 21\u001b[39m, in \u001b[36mcross_correlate_images\u001b[39m\u001b[34m(grouped_imgs)\u001b[39m\n\u001b[32m     19\u001b[39m         img_gray = color.rgb2gray(img)\n\u001b[32m     20\u001b[39m         img_features = feature.canny(img_gray)\n\u001b[32m---> \u001b[39m\u001b[32m21\u001b[39m         correlation = np.sum(\u001b[43mref_features\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m \u001b[49m\u001b[43mimg_features\u001b[49m)\n\u001b[32m     22\u001b[39m         correlated_images[(category, img_path.name)] = correlation\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m correlated_images\n",
      "\u001b[31mValueError\u001b[39m: operands could not be broadcast together with shapes (2983,3961) (2736,3648) "
     ]
    }
   ],
   "source": [
    "# Cross correlate each image within each category to find the most common part of the images\n",
    "from skimage import io, feature, color\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def cross_correlate_images(grouped_imgs: dict) -> dict:\n",
    "    correlated_images = {}\n",
    "    for category, img_paths in grouped_imgs.items():\n",
    "        if not img_paths:\n",
    "            continue\n",
    "        # Load the first image as a reference\n",
    "        ref_img = io.imread(img_paths[0])\n",
    "        ref_img_gray = color.rgb2gray(ref_img)\n",
    "        # Compute features for the reference image\n",
    "        ref_features = feature.canny(ref_img_gray)\n",
    "        # Cross-correlate with other images\n",
    "        for img_path in img_paths[1:]:\n",
    "            img = io.imread(img_path)\n",
    "            img_gray = color.rgb2gray(img)\n",
    "            img_features = feature.canny(img_gray)\n",
    "            correlation = np.sum(ref_features * img_features)\n",
    "            correlated_images[(category, img_path.name)] = correlation\n",
    "    return correlated_images\n",
    "correlated_results = cross_correlate_images(grouped_imgs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
