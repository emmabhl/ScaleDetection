{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7931fc3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'image': <PIL.Image.Image image mode=RGB size=2880x2160>}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"emmabhl/BIOSCAN_scalebar\",\n",
    "ds['train'][0]['image'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5398cab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "import torch\n",
    "import os\n",
    "from PIL import Image\n",
    "from src.prompt import PROMPT_TEMPLATE\n",
    "import argparse\n",
    "from datasets import load_dataset\n",
    "\n",
    "def VLM_scale_detection(\n",
    "    datapath: str = \"ds = load_dataset(datapath)\",\n",
    "    output_folder: str = \"outputs_vlm\",\n",
    "    model_id: str = \"Qwen/Qwen3-VL-4B-Instruct\",\n",
    "    max_side: int = 1024\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Perform scale detection using a Vision-Language Model (VLM).\n",
    "    \n",
    "    Args:\n",
    "        datapath (str): Path to the HuggingFace dataset or local directory containing images.\n",
    "        output_folder (str): Folder to save output JSON files.\n",
    "        model_id (str): The identifier of the pre-trained VLM model.\n",
    "        max_side (int): The maximum side length for image resizing to manage memory usage.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Load model (with lower-precision to save memory)\n",
    "    model = Qwen3VLForConditionalGeneration.from_pretrained(model_id, device_map=\"auto\", dtype=torch.float16)\n",
    "    processor = AutoProcessor.from_pretrained(model_id)\n",
    "\n",
    "    def resize_max_side(pil_img: Image.Image, max_side: int):\n",
    "        w, h = pil_img.size\n",
    "        if max(w, h) <= max_side:\n",
    "            return pil_img\n",
    "        # preserve aspect ratio\n",
    "        if w >= h:\n",
    "            new_w = max_side\n",
    "            new_h = int(h * (max_side / w))\n",
    "        else:\n",
    "            new_h = max_side\n",
    "            new_w = int(w * (max_side / h))\n",
    "        return pil_img.resize((new_w, new_h), resample=Image.LANCZOS)\n",
    "    \n",
    "    ds = load_dataset(datapath)\n",
    "        \n",
    "    for image_record in ds['train']:\n",
    "        image = image_record['image']\n",
    "    \n",
    "        # Resize to limit memory footprint (important!)\n",
    "        image = resize_max_side(image, max_side)\n",
    "\n",
    "        # Build messages same as before (image inserted by processor)\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": PROMPT_TEMPLATE},\n",
    "                ],\n",
    "            }\n",
    "        ]\n",
    "\n",
    "        # Option A: use apply_chat_template -> then processor(images=..., text=...)\n",
    "        text = processor.apply_chat_template(messages, add_generation_prompt=True)\n",
    "\n",
    "        # Create inputs (pixel_values will be resized/normalized by the processor)\n",
    "        # Note: processor returns a BatchEncoding; move tensors explicitly to model.device\n",
    "        inputs = processor(images=[image], text=text, return_tensors=\"pt\")\n",
    "        device = next(model.parameters()).device\n",
    "        inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "        # Generate output\n",
    "        with torch.no_grad():\n",
    "            generated_ids = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "        # Trim prompt portion correctly using sequence lengths\n",
    "        input_ids = inputs[\"input_ids\"]\n",
    "        # input_ids is shape (batch, seq_len)\n",
    "        seq_lens = input_ids.shape[1]\n",
    "        # For batch size 1:\n",
    "        generated_ids_trimmed = generated_ids[:, seq_lens:]\n",
    "        output_text = processor.batch_decode(\n",
    "            generated_ids_trimmed, skip_special_tokens=True, clean_up_tokenization_spaces=False\n",
    "        )[0]\n",
    "        \n",
    "        # Save the output in a json file in a new folder \"outputs_vlm\"\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        output_path = os.path.join(output_folder, filename + \".json\")\n",
    "        with open(output_path, \"w\") as f:\n",
    "            import json\n",
    "            json.dump(output_text, f, indent=2)\n",
    "            \n",
    "    \n",
    "VLM_scale_detection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{\\\"scale_bar_found\\\": true, \\\"bbox\\\": [x1, y1, x2, y2], \\\"measured_scale_length\\\": 290, \\\"declared_scale_length\\\": \\\"10\\\", \\\"units\\\": \\\"Âµm\\\", \\\"scale_bar_confidence\\\": 0.5331090092658997, \\\"text_label_confidence\\\": 0.7824959754943848}\"\n",
    "\n",
    "\"{\\\"scale_bar_found\\\": false, \\\"bbox\\\": [], \\\"measured_scale_length\\\": null, \\\"declared_scale_length\\\": null, \\\"units\\\": null, \\\"scale_bar_confidence\\\": 0.0, \\\"text_label_confidence\\\": 0.0}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
