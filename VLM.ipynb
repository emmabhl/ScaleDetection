{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20d0a981",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/miniconda3/envs/scale/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-21 14:02:26 [__init__.py:216] Automatically detected platform cpu.\n",
      "video_kwargs: {'do_sample_frames': False}\n",
      "INFO 10-21 14:02:35 [utils.py:233] non-default args: {'trust_remote_code': True, 'seed': 0, 'tensor_parallel_size': 0, 'gpu_memory_utilization': 0.7, 'disable_log_stats': True, 'model': 'Qwen/Qwen3-VL-4B-Instruct-FP8'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-21 14:02:42 [importing.py:63] Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
      "INFO 10-21 14:02:42 [model.py:547] Resolved architecture: Qwen3VLForConditionalGeneration\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 10-21 14:02:42 [model.py:1510] Using max model len 262144\n",
      "WARNING 10-21 14:02:42 [cpu.py:117] Environment variable VLLM_CPU_KVCACHE_SPACE (GiB) for CPU backend is not set, using 4 by default.\n",
      "INFO 10-21 14:02:42 [arg_utils.py:1166] Chunked prefill is not supported for ARM and POWER and S390X CPUs; disabling it for V1 backend.\n"
     ]
    },
    {
     "ename": "ValidationError",
     "evalue": "1 validation error for SchedulerConfig\n  Value error, max_num_batched_tokens (0) is smaller than max_model_len (262144). This effectively limits the maximum sequence length to max_num_batched_tokens and makes vLLM reject longer sequences. Please increase max_num_batched_tokens or decrease max_model_len. [type=value_error, input_value=ArgsKwargs((), {'runner_t...ync_scheduling': False}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValidationError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     63\u001b[39m processor = AutoProcessor.from_pretrained(checkpoint_path)\n\u001b[32m     64\u001b[39m inputs = [prepare_inputs_for_vllm(message, processor) \u001b[38;5;28;01mfor\u001b[39;00m message \u001b[38;5;129;01min\u001b[39;00m [messages]]\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m llm = \u001b[43mLLM\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     67\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrust_remote_code\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgpu_memory_utilization\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.70\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m    \u001b[49m\u001b[43menforce_eager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensor_parallel_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdevice_count\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43mseed\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\n\u001b[32m     73\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     75\u001b[39m sampling_params = SamplingParams(\n\u001b[32m     76\u001b[39m     temperature=\u001b[32m0\u001b[39m,\n\u001b[32m     77\u001b[39m     max_tokens=\u001b[32m1024\u001b[39m,\n\u001b[32m     78\u001b[39m     top_k=-\u001b[32m1\u001b[39m,\n\u001b[32m     79\u001b[39m     stop_token_ids=[],\n\u001b[32m     80\u001b[39m )\n\u001b[32m     82\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, input_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(inputs):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/scale/lib/python3.11/site-packages/vllm/entrypoints/llm.py:297\u001b[39m, in \u001b[36mLLM.__init__\u001b[39m\u001b[34m(self, model, runner, convert, tokenizer, tokenizer_mode, skip_tokenizer_init, trust_remote_code, allowed_local_media_path, allowed_media_domains, tensor_parallel_size, dtype, quantization, revision, tokenizer_revision, seed, gpu_memory_utilization, swap_space, cpu_offload_gb, enforce_eager, disable_custom_all_reduce, hf_token, hf_overrides, mm_processor_kwargs, pooler_config, override_pooler_config, structured_outputs_config, kv_cache_memory_bytes, compilation_config, logits_processors, **kwargs)\u001b[39m\n\u001b[32m    294\u001b[39m log_non_default_args(engine_args)\n\u001b[32m    296\u001b[39m \u001b[38;5;66;03m# Create the Engine (autoselects V0 vs V1)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m297\u001b[39m \u001b[38;5;28mself\u001b[39m.llm_engine = \u001b[43mLLMEngine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_engine_args\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    298\u001b[39m \u001b[43m    \u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m=\u001b[49m\u001b[43mengine_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m=\u001b[49m\u001b[43mUsageContext\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLLM_CLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    299\u001b[39m \u001b[38;5;28mself\u001b[39m.engine_class = \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m.llm_engine)\n\u001b[32m    301\u001b[39m \u001b[38;5;28mself\u001b[39m.request_counter = Counter()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/scale/lib/python3.11/site-packages/vllm/v1/engine/llm_engine.py:169\u001b[39m, in \u001b[36mLLMEngine.from_engine_args\u001b[39m\u001b[34m(cls, engine_args, usage_context, stat_loggers, enable_multiprocessing)\u001b[39m\n\u001b[32m    166\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Creates an LLM engine from the engine arguments.\"\"\"\u001b[39;00m\n\u001b[32m    168\u001b[39m \u001b[38;5;66;03m# Create the engine configs.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m vllm_config = \u001b[43mengine_args\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_engine_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43musage_context\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    170\u001b[39m executor_class = Executor.get_class(vllm_config)\n\u001b[32m    172\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m envs.VLLM_ENABLE_V1_MULTIPROCESSING:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/scale/lib/python3.11/site-packages/vllm/engine/arg_utils.py:1360\u001b[39m, in \u001b[36mEngineArgs.create_engine_config\u001b[39m\u001b[34m(self, usage_context, headless)\u001b[39m\n\u001b[32m   1357\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m speculative_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1358\u001b[39m     num_lookahead_slots = speculative_config.num_lookahead_slots\n\u001b[32m-> \u001b[39m\u001b[32m1360\u001b[39m scheduler_config = \u001b[43mSchedulerConfig\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1361\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrunner_type\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrunner_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1362\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_num_batched_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_num_batched_tokens\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1363\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_num_seqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1364\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmax_model_len\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1365\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcuda_graph_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcuda_graph_sizes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1366\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_lookahead_slots\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_lookahead_slots\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1367\u001b[39m \u001b[43m    \u001b[49m\u001b[43menable_chunked_prefill\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menable_chunked_prefill\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1368\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_chunked_mm_input\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisable_chunked_mm_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1369\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_multimodal_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_multimodal_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1370\u001b[39m \u001b[43m    \u001b[49m\u001b[43msend_delta_data\u001b[49m\u001b[43m=\u001b[49m\u001b[43m(\u001b[49m\u001b[43menvs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mVLLM_USE_RAY_SPMD_WORKER\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m                     \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mparallel_config\u001b[49m\u001b[43m.\u001b[49m\u001b[43muse_ray\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpolicy\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduling_policy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscheduler_cls\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscheduler_cls\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_num_partial_prefills\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_num_partial_prefills\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_long_partial_prefills\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_long_partial_prefills\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlong_prefill_token_threshold\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlong_prefill_token_threshold\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_hybrid_kv_cache_manager\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdisable_hybrid_kv_cache_manager\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43masync_scheduling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43masync_scheduling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m model_config.is_multimodal_model \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.default_mm_loras:\n\u001b[32m   1383\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1384\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDefault modality-specific LoRA(s) were provided for a \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1385\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mnon multimodal model\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/miniconda3/envs/scale/lib/python3.11/site-packages/pydantic/_internal/_dataclasses.py:123\u001b[39m, in \u001b[36mcomplete_dataclass.<locals>.__init__\u001b[39m\u001b[34m(__dataclass_self__, *args, **kwargs)\u001b[39m\n\u001b[32m    121\u001b[39m __tracebackhide__ = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    122\u001b[39m s = __dataclass_self__\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[43ms\u001b[49m\u001b[43m.\u001b[49m\u001b[43m__pydantic_validator__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_python\u001b[49m\u001b[43m(\u001b[49m\u001b[43mArgsKwargs\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mself_instance\u001b[49m\u001b[43m=\u001b[49m\u001b[43ms\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mValidationError\u001b[39m: 1 validation error for SchedulerConfig\n  Value error, max_num_batched_tokens (0) is smaller than max_model_len (262144). This effectively limits the maximum sequence length to max_num_batched_tokens and makes vLLM reject longer sequences. Please increase max_num_batched_tokens or decrease max_model_len. [type=value_error, input_value=ArgsKwargs((), {'runner_t...ync_scheduling': False}), input_type=ArgsKwargs]\n    For further information visit https://errors.pydantic.dev/2.11/v/value_error"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8317fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"{\\\"scale_bar_found\\\": true, \\\"bbox\\\": [x1, y1, x2, y2], \\\"measured_scale_length\\\": 290, \\\"declared_scale_length\\\": \\\"10\\\", \\\"units\\\": \\\"Âµm\\\", \\\"scale_bar_confidence\\\": 0.5331090092658997, \\\"text_label_confidence\\\": 0.7824959754943848}\"\n",
    "\n",
    "\"{\\\"scale_bar_found\\\": false, \\\"bbox\\\": [], \\\"measured_scale_length\\\": null, \\\"declared_scale_length\\\": null, \\\"units\\\": null, \\\"scale_bar_confidence\\\": 0.0, \\\"text_label_confidence\\\": 0.0}\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scale",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
